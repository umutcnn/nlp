{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7ac2ef6a-841e-46c3-9374-979c223797b5",
   "metadata": {},
   "source": [
    "#https://github.com/mohitgupta-1O1/Kaggle-SMS-Spam-Collection-Dataset-/blob/master/spam.csv\n",
    " \"\"\"\n",
    "spam veri seti -> spam ve ham -> binary classification with decision tree\n",
    " \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a572ce0-038c-4492-b3de-7f21ae0fc430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label    0\n",
      "text     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"spam.csv\", encoding =\"Latin-1\")\n",
    "data.drop([ 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace = True) \n",
    "data.columns = [\"Label\", \"text\"] \n",
    "# EDA: Kesifsel veri analizi: missing value(Kayıp değer var mı)\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3ed5a5f-1c64-4015-b487-6175f1f28aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\UMUT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\UMUT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\UMUT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# veri setini yuklemek icin nltk kutuphanesi kullaniliyor\n",
    "import nltk\n",
    "\n",
    "# \"stopwords\": \"the\", \"is\", \"in\" gibi metnin anlamina dogrudan katkisi olmayan, sik gecen kelimeleri barindiran listeyi indirir\n",
    "nltk.download(\"stopwords\")  # cok kullanilan ve anlam tasimayan kelimeleri metinden cikarmak icin\n",
    "\n",
    "# \"wordnet\": Ingilizce kelimelerin koklerine (lemma) ulasmak icin kullanilan bir sozluk/veritabani\n",
    "nltk.download(\"wordnet\")    # lemmatization islemi icin gerekli veriseti\n",
    "\n",
    "# \"omw-1.4\": Open Multilingual Wordnet; WordNet'in cok dilli versiyonudur\n",
    "nltk.download(\"omw-1.4\")     # wordnete ait farkli dillerin kelime anlamlarini iceren veriseti\n",
    "\n",
    "# Gerekli kutuphaneler import ediliyor\n",
    "import re  # duzenli ifadeler icin\n",
    "from nltk.corpus import stopwords  # stopwords listesine erismek icin\n",
    "from nltk.stem import WordNetLemmatizer  # lemmatization yapmak icin\n",
    "\n",
    "# ornek veri: \"data\" adli bir DataFrame'den metinleri aliyoruz\n",
    "text = list(data.text)  # data.text kolonundaki metinleri listeye ceviriyoruz\n",
    "lemmatizer = WordNetLemmatizer()  # lemmatizer nesnesi olusturuluyor\n",
    "\n",
    "corpus = []  # islenmis ve temizlenmis metinlerin tutulacagi bos bir liste\n",
    "\n",
    "# Tum metinleri tek tek islemek icin dongu\n",
    "for i in range(len(text)):\n",
    "    # Harf olmayan tum karakterleri sil (ornegin noktalama isaretleri, rakamlar vs.)\n",
    "    r = re.sub(\"[^a-zA-Z]\", \" \", text[i])\n",
    "\n",
    "    # Tum harfleri kucuk harfe cevir (case normalization)\n",
    "    r = r.lower()\n",
    "\n",
    "    # Metni kelimelere ayir\n",
    "    r = r.split()\n",
    "\n",
    "    # Stopwords (anlamsiz kelimeler) listesine gore filtrele\n",
    "    r = [word for word in r if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    # Her bir kelimeyi lemmatizer kullanarak kok haline getir\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    # Liste halindeki kelimeleri tekrar boslukla birlestirerek tek string haline getir\n",
    "    r = \" \".join(r)\n",
    "\n",
    "    # Sonuclari corpus listesine ekle\n",
    "    corpus.append(r)\n",
    "data[\"text2\"]  = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8223285f-e085-4d17-9711-7e916f8847ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Label', 'text', 'text2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# DataFrame kolonlarini yazdir (kontrol icin)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b1dc42-183d-462e-a3d0-73ff05ef6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model egitimi ve degerlendirmesi\n",
    "x = data[\"text2\"]  # ozellik olarak 'text2' kolonu seciliyor\n",
    "y = data[\"Label\"]  # hedef degisken 'Label'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Veriyi egitim ve test olarak ayir (80% egitim, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3074dfdc-2635-44bd-84e4-164ef8f27e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ozellik cikarma: Bag of Words yaklasimi\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "x_train_cv = cv.fit_transform(x_train)  # egitim verisi icin kelime sayimi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9eb5ef-225f-4610-b336-512f5d964b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sınıflandırıcı egitimi: Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train_cv, y_train)  # modeli egit\n",
    "\n",
    "# Test verisini de sayisal vektorlere cevir\n",
    "x_test_cv = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4ecdd36-2f91-4fae-974a-7592cea36193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[956   9]\n",
      " [ 24 126]]\n",
      "109099.19282511211\n"
     ]
    }
   ],
   "source": [
    "# Tahmin yap\n",
    "prediction = dt.predict(x_test_cv)\n",
    "\n",
    "# Karisiklik matrisini hesapla\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c_matrix = confusion_matrix(y_test, prediction)\n",
    "print(c_matrix)\n",
    "\n",
    "# Dogruluk (accuracy) hesapla\n",
    "accuracy = 100 * (sum(sum(c_matrix)) - c_matrix[1,0] - c_matrix[0,1]) / sum(sum(c_matrix))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9e4dd-696a-4d1e-a8f8-4f622019c77e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
